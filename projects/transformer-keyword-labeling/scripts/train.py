import yaml
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import json
import os, sys

# è§£å†³ç›¸å¯¹è·¯å¾„å¯¼å…¥é—®é¢˜
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from models.transformer_model import MiniTransformer

# è¶…å‚æ•°å®šä¹‰
MAX_VOCAB = 1000
label2id = {"O": 0, "B-ORG": 1, "I-ORG": 2}
PAD_LABEL_ID = -100

print("ğŸš€ Transformer Keyword Labeling Training Started")

# è¯»å–é…ç½®
with open("config/config.yaml") as f:
    config = yaml.safe_load(f)
print(f"ğŸ“„ Using data: {config['train_file']}")

# æ•°æ®é›†å®šä¹‰
class MyDataset(Dataset):
    def __init__(self, path):
        self.data = [json.loads(line) for line in open(path, encoding="utf-8")]

    def __getitem__(self, idx):
        x = self.data[idx]["text"]
        y = self.data[idx]["labels"]
        assert len(x) == len(y), f"æ ·æœ¬ {idx} çš„è¾“å…¥ä¸æ ‡ç­¾é•¿åº¦ä¸ä¸€è‡´"  # æ ¡éªŒæ•°æ®å¯¹é½
        x_tensor = torch.tensor([min(ord(c), MAX_VOCAB - 1) for c in x], dtype=torch.long)
        y_tensor = torch.tensor([label2id[label] for label in y], dtype=torch.long)
        return x_tensor, y_tensor

    def __len__(self):
        return len(self.data)

# ç»Ÿä¸€å¡«å……çš„ pad å‡½æ•°
def pad(batch):
    x, y = zip(*batch)
    max_len = max(max(len(xi) for xi in x), max(len(yi) for yi in y))
    x_pad = torch.stack([
        torch.cat([xi, torch.zeros(max_len - len(xi), dtype=xi.dtype)]) for xi in x
    ])
    y_pad = torch.stack([
        torch.cat([yi, torch.full((max_len - len(yi),), PAD_LABEL_ID, dtype=yi.dtype)]) for yi in y
    ])
    return x_pad, y_pad

# æ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    MyDataset(config["train_file"]),
    batch_size=2,
    shuffle=True,
    collate_fn=pad
)

# æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
model = MiniTransformer()
criterion = nn.CrossEntropyLoss(ignore_index=PAD_LABEL_ID)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# è®­ç»ƒå¾ªç¯
for epoch in range(2):
    for i, (x, y) in enumerate(train_loader):
        out = model(x)                          # [B, T, C]
        logits = out.view(-1, out.shape[-1])    # [B*T, C]
        labels = y.view(-1)                     # [B*T]
        mask = labels != PAD_LABEL_ID           # [B*T]

        logits = logits[mask]                   # [æœ‰æ•ˆä½ç½®, C]
        labels = labels[mask]                   # [æœ‰æ•ˆä½ç½®]

        loss = criterion(logits, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print(f"ğŸ“˜ Epoch {epoch+1} | Step {i+1} | Loss: {loss.item():.4f}")
        print(f"   â· logits shape: {logits.shape} | labels shape: {labels.shape}")

# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), "models/transformer.pt")
print("âœ… Model saved to models/transformer.pt")
